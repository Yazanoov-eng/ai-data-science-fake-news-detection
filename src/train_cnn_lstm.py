import argparse
import os
import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoModel
from tqdm import tqdm

from .config import TrainConfig
from .dataloaders import NewsDataset
from .models import CNNLSTMClassifier
from .evaluate import compute_metrics, save_json

def set_seed(seed: int):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

@torch.no_grad()
def extract_bert_embeddings(bert, batch, device):
    out = bert(
        input_ids=batch["input_ids"].to(device),
        attention_mask=batch["attention_mask"].to(device)
    )
    return out.last_hidden_state  # (B, T, E)

def run_epoch(model, bert, loader, optimizer, device, train: bool):
    model.train() if train else model.eval()
    loss_fn = nn.CrossEntropyLoss()

    all_y, all_pred = [], []
    total_loss = 0.0

    for batch in tqdm(loader, desc="train" if train else "eval"):
        labels = batch["labels"].to(device)

        with torch.no_grad():
            emb = extract_bert_embeddings(bert, batch, device)

        logits = model(emb)
        loss = loss_fn(logits, labels)

        if train:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        total_loss += loss.item() * labels.size(0)
        preds = torch.argmax(logits, dim=1)

        all_y.extend(labels.detach().cpu().tolist())
        all_pred.extend(preds.detach().cpu().tolist())

    metrics = compute_metrics(all_y, all_pred)
    metrics["loss"] = float(total_loss / len(all_y))
    return metrics

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dataset", type=str, choices=["isot","kaggle"], required=True)
    ap.add_argument("--data_dir", type=str, default="outputs")
    ap.add_argument("--epochs", type=int, default=3)
    ap.add_argument("--max_length", type=int, default=256)
    ap.add_argument("--batch_size", type=int, default=16)
    args = ap.parse_args()

    cfg = TrainConfig()
    cfg.epochs = args.epochs
    cfg.max_length = args.max_length
    cfg.batch_size = args.batch_size

    set_seed(cfg.seed)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    train_csv = os.path.join(args.data_dir, f"{args.dataset}_train.csv")
    val_csv   = os.path.join(args.data_dir, f"{args.dataset}_val.csv")
    test_csv  = os.path.join(args.data_dir, f"{args.dataset}_test.csv")

    train_ds = NewsDataset(train_csv, cfg.bert_model_name, cfg.max_length)
    val_ds   = NewsDataset(val_csv, cfg.bert_model_name, cfg.max_length)
    test_ds  = NewsDataset(test_csv, cfg.bert_model_name, cfg.max_length)

    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)
    val_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)
    test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)

    # Frozen BERT encoder (required: embeddings generated by BERT)
    bert = AutoModel.from_pretrained(cfg.bert_model_name).to(device)
    bert.eval()
    for p in bert.parameters():
        p.requires_grad = False

    embed_dim = bert.config.hidden_size
    model = CNNLSTMClassifier(
        embed_dim=embed_dim,
        num_filters=cfg.cnn_num_filters,
        kernel_sizes=cfg.cnn_kernel_sizes,
        lstm_hidden_size=cfg.lstm_hidden_size,
        lstm_num_layers=cfg.lstm_num_layers,
        dropout=cfg.dropout
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

    best_val_f1 = -1.0
    best_path = os.path.join(args.data_dir, f"cnn_lstm_{args.dataset}.pt")

    for epoch in range(1, cfg.epochs + 1):
        print(f"\nEpoch {epoch}/{cfg.epochs} | device={device}")
        tr = run_epoch(model, bert, train_loader, optimizer, device, train=True)
        va = run_epoch(model, bert, val_loader, optimizer, device, train=False)

        print("Train:", tr)
        print("Val  :", va)

        if va["f1"] > best_val_f1:
            best_val_f1 = va["f1"]
            torch.save(model.state_dict(), best_path)
            print(f"[OK] Saved best model -> {best_path}")

    model.load_state_dict(torch.load(best_path, map_location=device))
    te = run_epoch(model, bert, test_loader, optimizer, device, train=False)
    print("\nTest:", te)

    out = {
        "model": "CNN+LSTM on frozen BERT embeddings",
        "dataset": args.dataset,
        "best_val_f1": best_val_f1,
        "test": te
    }
    out_path = os.path.join(args.data_dir, f"results_cnn_lstm_{args.dataset}.json")
    save_json(out, out_path)
    print(f"[OK] Wrote results -> {out_path}")

if __name__ == "__main__":
    main()
